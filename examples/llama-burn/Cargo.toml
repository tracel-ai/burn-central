[package]
authors = ["guillaumelagrange <lagrange.guillaume.1@gmail.com>"]
license = "MIT OR Apache-2.0"
name = "llama-burn"
version = "0.1.0"
edition = "2021"
description = "Llama 3 large language model with Burn"

[features]
default = ["pretrained", "cuda"]
pretrained = ["burn/network", "dep:dirs"]

llama3 = ["dep:tiktoken-rs", "dep:rustc-hash", "dep:base64"]
tiny = ["dep:tokenizers"]

# Example feature flags (backend selection)
tch-cpu = ["burn/tch"]
tch-gpu = ["burn/tch"]
cuda = ["burn/cuda"]
vulkan = ["burn/vulkan"]

# To import pytorch weights
import = ["burn-import"]

[dependencies]
burn-central = { workspace = true }
burn-central-runtime = { workspace = true }
burn = { workspace = true, default-features = false, features = ["std"] }
burn-import = { version = "0.20.0-pre.4", optional = true }
tracing = { version = "0.1.40", default-features = false, features = ["std"] }

itertools = { version = "0.12.1", default-features = false, features = [
    "use_alloc",
] }
dirs = { version = "5.0.1", optional = true }
serde = { version = "1.0.192", default-features = false, features = [
    "derive",
    "alloc",
] } # alloc is for no_std, derive is needed

# Tiktoken tokenizer (llama 3)
tiktoken-rs = { version = "0.5.8", optional = true }
base64 = { version = "0.22.1", optional = true }
rustc-hash = { version = "1.1.0", optional = true }

# SentencePiece tokenizer (tiny llama / llama 2)
tokenizers = { version = "0.19.1", default-features = false, features = [
    "onig",
], optional = true }

rand = { version = "0.8.5", default-features = false, features = [
    "alloc",
    "std_rng",
] } # std_rng is for no_std

crossbeam-channel = { version = "0.5.15" }

[dev-dependencies]
burn = { workspace = true, default-features = false }
clap = { version = "4.5.4", features = ["derive"] }
async-stream = { version = "0.3.5" }
axum = { version = "0.7.5" }
futures-util = { version = "0.3.30" }
reqwest = { version = "0.12.7", default-features = false, features = [
    "json",
    "stream",
    "rustls-tls",
] }
serde_json = { version = "1.0.117" }
tokio = { version = "1.38.0", features = [
    "macros",
    "rt-multi-thread",
    "sync",
    "time",
    "net",
] }
tracing-subscriber = { version = "0.3.18", features = ["env-filter"] }

[[example]]
name = "llama-burn-inference"
path = "examples/inference_server.rs"
